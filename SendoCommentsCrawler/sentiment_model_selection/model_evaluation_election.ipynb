{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from underthesea import sent_tokenize, word_tokenize, sentiment\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_id(sendo_url):\n",
    "    # Extract the product ID from the URL\n",
    "    parsed_url = urlparse(sendo_url)\n",
    "    # Split the path by '-'\n",
    "    path_parts = parsed_url.path.split('-')\n",
    "    # Get the last part of the path, remove '.html', and replace 'p' with ''\n",
    "    product_id = path_parts[-1].replace('.html', '').replace('p', '')\n",
    "    return product_id\n",
    "\n",
    "#Hàm tạo ra cấu trúc để lưu dữ liệu sau khi crawl data\n",
    "def comment_parser(json):\n",
    "    d = dict()\n",
    "    d['id'] = json.get('rating_id')\n",
    "    d['title'] = json.get('comment_title')\n",
    "    d['comment'] = json.get('comment')\n",
    "    d['default_sentiment'] = json.get('status')\n",
    "    d['like_count'] = json.get('like_count')\n",
    "    d['customer_id'] = json.get('customer_id')\n",
    "    d['rating_star'] = json.get('star')\n",
    "    d['customer_name'] = json.get('user_name')\n",
    "    return d\n",
    "\n",
    "#Hàm lấy comment \n",
    "def get_comments(product_id):\n",
    "    \"\"\"Fetches and parses comments for a given Tiki.vn product ID, including all pages, dropping duplicates.\"\"\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'vi,vi-VN;q=0.9,fr-FR;q=0.8,fr;q=0.7,en-US;q=0.6,en;q=0.5',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'page': 1,\n",
    "        'limit': 10,  # Adjust limit as needed\n",
    "        'sort': 'review_score',\n",
    "        'v': '2',\n",
    "        'star': 'all'\n",
    "    }\n",
    "\n",
    "    result = []\n",
    "    while True:\n",
    "        response = requests.get(f'https://ratingapi.sendo.vn/product/{product_id}/rating',headers=headers,params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json().get('data')\n",
    "            if not data:  # Check if there are no more comments\n",
    "                break\n",
    "            for comment in data:\n",
    "                parsed_comment = comment_parser(comment)\n",
    "                # Check if comment ID already exists in results before adding\n",
    "                if parsed_comment['id'] not in [c['id'] for c in result]:\n",
    "                    result.append(parsed_comment)\n",
    "            params['page'] += 1\n",
    "        else:\n",
    "            print(f\"Error getting comments for page {params['page']}. Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    df_comment = pd.DataFrame(result)\n",
    "    return df_comment\n",
    "\n",
    "#Hàm standardize_comment để chuẩn hóa comment trước khi Sentiment Analysis:\n",
    "def standardize_comment(comment):\n",
    "    comment = comment.replace('\\n', ' ')\\\n",
    "                    .replace('\\r', ' ')\\\n",
    "                    .replace('\"', ' ').replace(\"”\", \" \")\\\n",
    "                    .replace(\":\", \" \")\\\n",
    "                    .replace(\"!\", \" \")\\\n",
    "                    .replace(\"?\", \" \") \\\n",
    "                    .replace(\"-\", \" \")\\\n",
    "                    .replace(\"?\", \" \")\\\n",
    "                    .lower()\n",
    "    return comment\n",
    "\n",
    "\n",
    "#Hàm xóa Emoji ra khỏi comment\n",
    "def demoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "# Hàm để tokenize từng comment\n",
    "def tokenize_comment(comment):\n",
    "    # Tách câu\n",
    "    sentences = sent_tokenize(comment)\n",
    "    # Tách từ trong mỗi câu và lưu kết quả\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        # Xóa dấu câu khỏi mỗi từ\n",
    "        for i, word in enumerate(words):\n",
    "            words[i] = re.sub(r'[^\\w\\s]', '', word)\n",
    "\n",
    "        tokenized_sentences.append(words)\n",
    "    return tokenized_sentences\n",
    "\n",
    "#Hàm sentiment bằng Underthesea cho từng dòng trong cột comment\n",
    "def get_sentiment_by_underthesea(text):\n",
    "    sentiment_result = sentiment(text)\n",
    "    if sentiment_result is None:\n",
    "        return 'NEUTRAL'\n",
    "    elif 'positive' in sentiment_result:\n",
    "        return 'POSITIVE'\n",
    "    elif 'negative' in sentiment_result:\n",
    "        return 'NEGATIVE'\n",
    "    else:\n",
    "        return 'NEUTRAL'\n",
    "\n",
    "#Hàm sentiment bằng PhoBert cho từng dòng trong cột comment:\n",
    "def get_sentiment_scores_by_phobert(text):\n",
    "  \n",
    "  checkpoint = \"mr4/phobert-base-vi-sentiment-analysis\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "  inputs = tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "  outputs = model(**inputs)\n",
    "  predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "  scores = predictions[0].tolist()  # Get scores for the first input (single text)\n",
    "  highest_score_index = scores.index(max(scores))  # Find index of highest score\n",
    "  label_mapping = model.config.id2label  # Get label mapping from model config\n",
    "  dominant_sentiment = label_mapping[highest_score_index]\n",
    "  return dominant_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment phần code này để tạo ra một bộ dataset gồm đa dạng các comment từ các sản phẩm khác nhau\n",
    "# Cứ chạy liên tục phần với nhiều URL khác nhau\n",
    "# <<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>\n",
    "#Lấy commment từ URL\n",
    "def parallel_apply(df, func, n_jobs=-10):\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(func)(row) for row in df)\n",
    "    return results\n",
    "\n",
    "link = input('Nhập vào URL Sendo: ')\n",
    "product_id = get_product_id(link)\n",
    "#Lấy comment từ product_id\n",
    "df = get_comments(product_id)\n",
    "\n",
    "#Xử lý xóa duplicate và emojis\n",
    "df = df.drop_duplicates(subset='id', keep='first') # Xóa duplicates dựa trên cột 'id'\n",
    "df['comment'] = df['comment'].apply(demoji) #Xóa emoji\n",
    "df['comment'] = df['comment'].apply(standardize_comment)\n",
    "df.to_csv('D:\\Personal Projects\\SendoCommentsCrawler\\model_evaluation\\dataset\\dataset.csv', mode='a', encoding='utf-8-sig')\n",
    "\n",
    "#Copy dataframe df vào df_comment\n",
    "df_comment = df.copy()\n",
    "df_comment['comment'] = df_comment['comment'].apply(str)\n",
    "\n",
    "#Tokennize cột 'comment' để tạo ra cột mới 'tokenized_comment' -> output là các mảng\n",
    "df_comment['tokenized_comment'] = df_comment['comment'].apply(tokenize_comment)\n",
    "\n",
    "#Tách các mảng sau khi được tokenize thành từng dòng riêng biệt\n",
    "df_comment = df_comment.explode('tokenized_comment')\n",
    "\n",
    "# Xóa các dòng rỗng trong cột 'tokenized_comment'\n",
    "df_comment.dropna(subset=['tokenized_comment'], inplace=True)\n",
    "\n",
    "# Ghép các mảng trong cột 'tokenized_comment' thành text theo từng dòng\n",
    "df_comment['tokenized_comment'] = df_comment['tokenized_comment'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Apply sentiment analysis in parallel\n",
    "df_comment['underthesea_sentiment'] = parallel_apply(df_comment['comment'], get_sentiment_by_underthesea)\n",
    "df_comment['phobert_sentiment_score'] = parallel_apply(df_comment['comment'], get_sentiment_scores_by_phobert)\n",
    "\n",
    "\n",
    "df_comment.to_csv('D:\\Personal Projects\\SendoCommentsCrawler\\model_evaluation\\output_model_dataset\\model_comparision_dataset.csv',encoding='utf-8-sig')\n",
    "\n",
    "df_comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
