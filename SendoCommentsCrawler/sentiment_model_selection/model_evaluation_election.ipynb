{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from underthesea import sent_tokenize, word_tokenize, sentiment\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_id(sendo_url):\n",
    "    # Extract the product ID from the URL\n",
    "    parsed_url = urlparse(sendo_url)\n",
    "    # Split the path by '-'\n",
    "    path_parts = parsed_url.path.split('-')\n",
    "    # Get the last part of the path, remove '.html', and replace 'p' with ''\n",
    "    product_id = path_parts[-1].replace('.html', '').replace('p', '')\n",
    "    return product_id\n",
    "\n",
    "#Hàm tạo ra cấu trúc để lưu dữ liệu sau khi crawl data\n",
    "def comment_parser(json):\n",
    "    d = dict()\n",
    "    d['id'] = json.get('rating_id')\n",
    "    d['title'] = json.get('comment_title')\n",
    "    d['comment'] = json.get('comment')\n",
    "    d['default_sentiment'] = json.get('status')\n",
    "    d['like_count'] = json.get('like_count')\n",
    "    d['customer_id'] = json.get('customer_id')\n",
    "    d['rating_star'] = json.get('star')\n",
    "    d['customer_name'] = json.get('user_name')\n",
    "    return d\n",
    "\n",
    "#Hàm lấy comment \n",
    "def get_comments(product_id):\n",
    "    \"\"\"Fetches and parses comments for a given Tiki.vn product ID, including all pages, dropping duplicates.\"\"\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'vi,vi-VN;q=0.9,fr-FR;q=0.8,fr;q=0.7,en-US;q=0.6,en;q=0.5',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'page': 1,\n",
    "        'limit': 10,  # Adjust limit as needed\n",
    "        'sort': 'review_score',\n",
    "        'v': '2',\n",
    "        'star': 'all'\n",
    "    }\n",
    "\n",
    "    result = []\n",
    "    while True:\n",
    "        response = requests.get(f'https://ratingapi.sendo.vn/product/{product_id}/rating',headers=headers,params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json().get('data')\n",
    "            if not data:  # Check if there are no more comments\n",
    "                break\n",
    "            for comment in data:\n",
    "                parsed_comment = comment_parser(comment)\n",
    "                # Check if comment ID already exists in results before adding\n",
    "                if parsed_comment['id'] not in [c['id'] for c in result]:\n",
    "                    result.append(parsed_comment)\n",
    "            params['page'] += 1\n",
    "        else:\n",
    "            print(f\"Error getting comments for page {params['page']}. Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    df_comment = pd.DataFrame(result)\n",
    "    return df_comment\n",
    "\n",
    "#Hàm standardize_comment để chuẩn hóa comment trước khi Sentiment Analysis:\n",
    "def standardize_comment(comment):\n",
    "    comment = comment.replace('\\n', ' ')\\\n",
    "                    .replace('\\r', ' ')\\\n",
    "                    .replace('\"', ' ').replace(\"”\", \" \")\\\n",
    "                    .replace(\":\", \" \")\\\n",
    "                    .replace(\"!\", \" \")\\\n",
    "                    .replace(\"?\", \" \") \\\n",
    "                    .replace(\"-\", \" \")\\\n",
    "                    .replace(\"?\", \" \")\\\n",
    "                    .lower()\n",
    "    return comment\n",
    "\n",
    "\n",
    "#Hàm xóa Emoji ra khỏi comment\n",
    "def demoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "# Hàm để tokenize từng comment\n",
    "def tokenize_comment(comment):\n",
    "    # Tách câu\n",
    "    sentences = sent_tokenize(comment)\n",
    "    # Tách từ trong mỗi câu và lưu kết quả\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        # Xóa dấu câu khỏi mỗi từ\n",
    "        for i, word in enumerate(words):\n",
    "            words[i] = re.sub(r'[^\\w\\s]', '', word)\n",
    "\n",
    "        tokenized_sentences.append(words)\n",
    "    return tokenized_sentences\n",
    "\n",
    "#Hàm sentiment bằng Underthesea cho từng dòng trong cột comment\n",
    "def get_sentiment_by_underthesea(text):\n",
    "    sentiment_result = sentiment(text)\n",
    "    if sentiment_result is None:\n",
    "        return 'NEUTRAL'\n",
    "    elif 'positive' in sentiment_result:\n",
    "        return 'POSITIVE'\n",
    "    elif 'negative' in sentiment_result:\n",
    "        return 'NEGATIVE'\n",
    "    else:\n",
    "        return 'NEUTRAL'\n",
    "\n",
    "#Hàm sentiment bằng PhoBert cho từng dòng trong cột comment:\n",
    "def get_sentiment_scores_by_phobert(text):\n",
    "  \n",
    "  checkpoint = \"mr4/phobert-base-vi-sentiment-analysis\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "  inputs = tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "  outputs = model(**inputs)\n",
    "  predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "  scores = predictions[0].tolist()  # Get scores for the first input (single text)\n",
    "  highest_score_index = scores.index(max(scores))  # Find index of highest score\n",
    "  label_mapping = model.config.id2label  # Get label mapping from model config\n",
    "  dominant_sentiment = label_mapping[highest_score_index]\n",
    "  return dominant_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment phần code này để tạo ra một bộ dataset gồm đa dạng các comment từ các sản phẩm khác nhau\n",
    "# <<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>\n",
    "#Lấy commment từ URL\n",
    "def parallel_apply(df, func, n_jobs=-10):\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(func)(row) for row in df)\n",
    "    return results\n",
    "\n",
    "link = input('Nhập vào URL Sendo: ')\n",
    "product_id = get_product_id(link)\n",
    "#Lấy comment từ product_id\n",
    "df = get_comments(product_id)\n",
    "\n",
    "#Xử lý xóa duplicate và emojis\n",
    "df = df.drop_duplicates(subset='id', keep='first') # Xóa duplicates dựa trên cột 'id'\n",
    "df['comment'] = df['comment'].apply(demoji) #Xóa emoji\n",
    "df['comment'] = df['comment'].apply(standardize_comment)\n",
    "df.to_csv('D:\\Personal Projects\\SendoCommentsCrawler\\model_evaluation\\dataset\\dataset.csv', mode='a', encoding='utf-8-sig')\n",
    "\n",
    "#Copy dataframe df vào df_comment\n",
    "df_comment = df.copy()\n",
    "df_comment['comment'] = df_comment['comment'].apply(str)\n",
    "\n",
    "#Tokennize cột 'comment' để tạo ra cột mới 'tokenized_comment' -> output là các mảng\n",
    "df_comment['tokenized_comment'] = df_comment['comment'].apply(tokenize_comment)\n",
    "\n",
    "#Tách các mảng sau khi được tokenize thành từng dòng riêng biệt\n",
    "df_comment = df_comment.explode('tokenized_comment')\n",
    "\n",
    "# Xóa các dòng rỗng trong cột 'tokenized_comment'\n",
    "df_comment.dropna(subset=['tokenized_comment'], inplace=True)\n",
    "\n",
    "# Ghép các mảng trong cột 'tokenized_comment' thành text theo từng dòng\n",
    "df_comment['tokenized_comment'] = df_comment['tokenized_comment'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Apply sentiment analysis in parallel\n",
    "df_comment['underthesea_sentiment'] = parallel_apply(df_comment['comment'], get_sentiment_by_underthesea)\n",
    "df_comment['phobert_sentiment_score'] = parallel_apply(df_comment['comment'], get_sentiment_scores_by_phobert)\n",
    "\n",
    "\n",
    "df_comment.to_csv('D:\\Personal Projects\\SendoCommentsCrawler\\model_evaluation\\output_model_dataset\\model_comparision_dataset.csv',encoding='utf-8-sig')\n",
    "\n",
    "df_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This TfidfTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhay qua\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\underthesea\\pipeline\\sentiment\\__init__.py:33\u001b[0m, in \u001b[0;36msentiment\u001b[1;34m(X, domain)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m domain \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munderthesea\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentiment\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m domain \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbank\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munderthesea\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentiment\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\underthesea\\pipeline\\sentiment\\general\\__init__.py:23\u001b[0m, in \u001b[0;36msentiment\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     20\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m TextClassifier\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m     22\u001b[0m sentence \u001b[38;5;241m=\u001b[39m Sentence(X)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m labels \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mlabels\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\underthesea\\models\\text_classifier.py:97\u001b[0m, in \u001b[0;36mTextClassifier.predict\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator \u001b[38;5;241m==\u001b[39m TEXT_CLASSIFIER_ESTIMATOR\u001b[38;5;241m.\u001b[39mPIPELINE:\n\u001b[0;32m     96\u001b[0m     text \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m---> 97\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultilabel:\n\u001b[0;32m     99\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_encoder\u001b[38;5;241m.\u001b[39minverse_transform(y)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\pipeline.py:603\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 603\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    606\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2118\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2115\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2117\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtransform(raw_documents)\n\u001b[1;32m-> 2118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1689\u001b[0m, in \u001b[0;36mTfidfTransformer.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform a count matrix to a tf or tf-idf representation.\u001b[39;00m\n\u001b[0;32m   1674\u001b[0m \n\u001b[0;32m   1675\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1689\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1691\u001b[0m         X,\n\u001b[0;32m   1692\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1695\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1696\u001b[0m     )\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:1632\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1632\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This TfidfTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "sentiment('hay qua')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
